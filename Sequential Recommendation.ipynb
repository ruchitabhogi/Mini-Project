{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["cd1bMG6jckat"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["upload files in paths for running this code\n","\n","> /content/amazon.csv\n","\n","> /content/datasets/books/seq\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"wflUB64LfK9l"}},{"cell_type":"markdown","source":["# **clone**"],"metadata":{"id":"adGYSke0HNrm"}},{"cell_type":"code","source":["!git clone https://github.com/HKUDS/MAERec.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"stKLo315HP3G","outputId":"fb75af2e-a9a5-4cd5-ba05-14559413a87e","executionInfo":{"status":"ok","timestamp":1724997410308,"user_tz":-330,"elapsed":9516,"user":{"displayName":"Ruchita Bhogi","userId":"02603965711634267794"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'MAERec'...\n","remote: Enumerating objects: 57, done.\u001b[K\n","remote: Counting objects: 100% (38/38), done.\u001b[K\n","remote: Compressing objects: 100% (37/37), done.\u001b[K\n","remote: Total 57 (delta 18), reused 4 (delta 1), pack-reused 19 (from 1)\u001b[K\n","Receiving objects: 100% (57/57), 81.24 MiB | 14.40 MiB/s, done.\n","Resolving deltas: 100% (20/20), done.\n"]}]},{"cell_type":"code","source":["!mkdir datasets"],"metadata":{"id":"HGUn-wyyJYQS","executionInfo":{"status":"ok","timestamp":1724997413761,"user_tz":-330,"elapsed":573,"user":{"displayName":"Ruchita Bhogi","userId":"02603965711634267794"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6060f5cd-1353-4b42-a616-90ef56501fdd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘datasets’: File exists\n"]}]},{"cell_type":"markdown","source":["# **streamlit**"],"metadata":{"id":"mTrg5nI-HHJq"}},{"cell_type":"code","source":["pip install streamlit -q"],"metadata":{"id":"kGdEF0WdWiGK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"af47fa05-9c59-4dd3-df92-9e183ba6a4ec","executionInfo":{"status":"ok","timestamp":1724997428409,"user_tz":-330,"elapsed":6605,"user":{"displayName":"Ruchita Bhogi","userId":"02603965711634267794"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/8.7 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m156.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["# app.py code just for reference"],"metadata":{"id":"cd1bMG6jckat"}},{"cell_type":"markdown","source":["app.py\n","\n","```\n","import streamlit as st\n","import logger as logger\n","from params import args\n","from logger import log\n","from handler import *\n","from model import *\n","from utils import *\n","import numpy as np\n","import torch as t\n","import pandas as pd\n","import pickle\n","import sys\n","import os\n","\n","\n","t.manual_seed(args.seed)\n","np.random.seed(args.seed)\n","\n","class Coach:\n","    def __init__(self, handler):\n","        self.handler = handler\n","\n","        log(f\"Users: {args.user}, Items(+1): {args.item}\")\n","        self.metrics = dict()\n","        mets = ['loss', 'loss_main', 'hr@10', 'ndcg@10']\n","        for met in mets:\n","            self.metrics['Train' + met] = list()\n","            self.metrics['Test' + met] = list()\n","\n","    def make_print(self, name, ep, reses, save):\n","        ret = 'Epoch %d/%d, %s: ' % (ep, args.epoch, name)\n","        for metric in reses:\n","            val = reses[metric]\n","            ret += '%s = %.4f, ' % (metric, val)\n","            tem = name + metric\n","            if save and tem in self.metrics:\n","                self.metrics[tem].append(val)\n","        ret = ret[:-2] + '                   '\n","        return ret\n","\n","    def run(self):\n","      self.prepare_model()\n","      log('Model Prepared')\n","      if args.load_model != None:\n","          self.load_model()\n","          stloc = len(self.metrics['Trainloss']) * args.test_frequency - (args.test_frequency - 1)\n","      else:\n","          stloc = 0\n","          log('Model Initialized')\n","\n","      bestRes = {'hr@10': 0, 'ndcg@10': 0}  # Initialize with initial values\n","\n","      reses = self.test_epoch()\n","      for ep in range(stloc, args.epoch):\n","          tst_flag = (ep % args.test_frequency == 0)\n","          reses = self.train_epoch()\n","          log(self.make_print('Train', ep, reses, tst_flag))\n","          sys.stdout.flush()\n","          if tst_flag:\n","              reses = self.test_epoch()\n","              log(self.make_print('Test', ep, reses, tst_flag))\n","              sys.stdout.flush()\n","              if bestRes is None or reses['hr@10'] > bestRes['hr@10']:\n","                  bestRes = reses\n","                  log(self.make_print('Best Result', args.epoch, bestRes, True), bold=True)\n","                  self.save_history()\n","          print()\n","\n","      reses = self.test_epoch()\n","      log(self.make_print('Test', args.epoch, reses, True))\n","      log(self.make_print('Best Result', args.epoch, bestRes, True), bold=True)\n","\n","    def prepare_model(self):\n","        self.encoder = Encoder().cuda()\n","        self.decoder = Decoder().cuda()\n","        self.recommender = SASRec().cuda()\n","        self.masker = RandomMaskSubgraphs()\n","        self.sampler = LocalGraph()\n","        self.opt = t.optim.Adam(\n","            [{\"params\": self.encoder.parameters()},\n","             {\"params\": self.decoder.parameters()},\n","             {\"params\": self.recommender.parameters()}],\n","            lr=args.lr, weight_decay=0\n","        )\n","\n","    def sample_pos_edges(self, masked_edges):\n","        return masked_edges[t.randperm(masked_edges.shape[0])[:args.con_batch]]\n","\n","    def sample_neg_edges(self, pos, dok):\n","        neg = []\n","        for u, v in pos:\n","            cu_neg = []\n","            num_samp = args.num_reco_neg // 2\n","            for i in range(num_samp):\n","                while True:\n","                    v_neg = np.random.randint(1, args.item)\n","                    if (u, v_neg) not in dok:\n","                        break\n","                cu_neg.append([u, v_neg])\n","            for i in range(num_samp):\n","                while True:\n","                    u_neg = np.random.randint(1, args.item)\n","                    if (u_neg, v) not in dok:\n","                        break\n","                cu_neg.append([u_neg, v])\n","            neg.append(cu_neg)\n","        return t.Tensor(neg).long()\n","\n","    def train_epoch(self):\n","        self.encoder.train()\n","        self.decoder.train()\n","        self.recommender.train()\n","        self.masker.train()\n","        self.sampler.train()\n","\n","        loss_his = []\n","        ep_loss, ep_loss_main, ep_loss_reco, ep_loss_mask = 0, 0, 0, 0\n","        trn_loader = self.handler.trn_loader\n","        steps = trn_loader.dataset.__len__() // args.trn_batch\n","\n","        for i, batch_data in enumerate(trn_loader):\n","\n","            if i % args.mask_steps == 0:\n","                sample_scr, candidates = self.sampler(self.handler.ii_adj_all_one, self.encoder.get_ego_embeds())\n","                masked_adj, masked_edg = self.masker(self.handler.ii_adj, candidates)\n","\n","            batch_data = [i.cuda() for i in batch_data]\n","            seq, pos, neg = batch_data\n","\n","            item_emb, item_emb_his = self.encoder(masked_adj)\n","            seq_emb = self.recommender(seq, item_emb)\n","            tar_msk = pos > 0\n","            loss_main = cross_entropy(seq_emb, item_emb[pos], item_emb[neg], tar_msk)\n","\n","            pos = self.sample_pos_edges(masked_edg)\n","            neg = self.sample_neg_edges(pos, self.handler.ii_dok)\n","            loss_reco = self.decoder(item_emb_his, pos, neg)\n","\n","            loss_regu = (calc_reg_loss(self.encoder) + calc_reg_loss(self.decoder) + calc_reg_loss(self.recommender)) * args.reg\n","\n","            loss = loss_main + loss_reco + loss_regu\n","            loss_his.append(loss_main)\n","\n","            if i % args.mask_steps == 0:\n","                reward = calc_reward(loss_his, args.eps)\n","                loss_mask = -sample_scr.mean() * reward\n","                ep_loss_mask += loss_mask\n","                loss_his = loss_his[-1:]\n","                loss += loss_mask\n","\n","            ep_loss += loss.item()\n","            ep_loss_main += loss_main.item()\n","            ep_loss_reco += loss_reco.item()\n","            log('Step %d/%d: loss = %.3f, loss_main = %.3f loss_regu = %.3f, loss_reco = %.3f        ' % (i, steps, loss, loss_main, loss_regu, loss_reco), save=False, oneline=True)\n","            sys.stdout.flush()\n","\n","            self.opt.zero_grad()\n","            loss.backward()\n","            self.opt.step()\n","\n","        ret = dict()\n","        ret['loss'] = ep_loss / steps\n","        ret['loss_main'] = ep_loss_main / steps\n","        ret['loss_reco'] = ep_loss_reco / steps\n","        ret['loss_mask'] = ep_loss_mask / (steps // args.mask_steps)\n","\n","        return ret\n","\n","    def test_epoch(self):\n","        self.encoder.eval()\n","        self.decoder.eval()\n","        self.recommender.eval()\n","        self.masker.eval()\n","        self.sampler.eval()\n","\n","        tst_loader = self.handler.tst_loader\n","        ep_h5, ep_n5, ep_h10, ep_n10, ep_h20, ep_n20, ep_h50, ep_n50  = [0] * 8\n","        group_h20 = [0] * 4\n","        group_n20 = [0] * 4\n","        group_num = [0] * 4\n","        num = tst_loader.dataset.__len__()\n","        steps = num // args.tst_batch\n","\n","        with t.no_grad():\n","            for i, batch_data in enumerate(tst_loader):\n","                batch_data = [i.cuda() for i in batch_data]\n","                seq, pos, neg = batch_data\n","                item_emb, item_emb_his = self.encoder(self.handler.ii_adj)\n","                seq_emb = self.recommender(seq, item_emb)\n","                seq_emb = seq_emb[:,-1,:] # (batch, 1, latdim)\n","                all_ids = t.cat([pos, neg], -1) # (batch, 100)\n","                all_emb = item_emb[all_ids] # (batch, 100, latdim)\n","                all_scr = t.sum(t.unsqueeze(seq_emb, 1) * all_emb, -1) # (batch, 100)\n","                seq_len = (seq > 0).cpu().numpy().sum(-1)\n","                h5, n5, h10, n10, h20, n20, h50, n50, gp_h20, gp_n20, gp_num= \\\n","                    self.calc_res(all_scr.cpu().numpy(), all_ids.cpu().numpy(), pos.cpu().numpy(), seq_len)\n","                ep_h5 += h5\n","                ep_n5 += n5\n","                ep_h10 += h10\n","                ep_n10 += n10\n","                ep_h20 += h20\n","                ep_n20 += n20\n","                ep_h50 += h50\n","                ep_n50 += n50\n","                for j in range(4):\n","                    group_h20[j] += gp_h20[j]\n","                    group_n20[j] += gp_n20[j]\n","                    group_num[j] += gp_num[j]\n","                log('Steps %d/%d: hr@10 = %.2f, ndcg@10 = %.2f          ' % (i, steps, h10, n10), save=False, oneline=True)\n","                sys.stdout.flush()\n","\n","        ep_h5 /= num\n","        ep_n5 /= num\n","        ep_h10 /= num\n","        ep_n10 /= num\n","        ep_h20 /= num\n","        ep_n20 /= num\n","        ep_h50 /= num\n","        ep_n50 /= num\n","\n","        for i in range(4):\n","            group_h20[i] /= group_num[i]\n","            group_n20[i] /= group_num[i]\n","\n","        ret = dict()\n","        ret['hr@10'] = ep_h10\n","        ret['ndcg@10'] = ep_n10\n","\n","        print(f'Test result: h5={ep_h5:.4f} n5={ep_n5:.4f} h10={ep_h10:.4f} n10={ep_n10:.4f} h20={ep_h20:.4f} n20={ep_n20:.4f} h50={ep_h50:.4f} n50={ep_n50:.4f}')\n","\n","        return ret\n","\n","    def calc_res(self, scores, tst_ids, pos_ids, seq_len):\n","        group_h20 = [0] * 4\n","        group_n20 = [0] * 4\n","        group_num = [0] * 4\n","        h5, n5, h10, n10, h20, n20, h50, n50 = [0] * 8\n","        for i in range(len(pos_ids)):\n","            ids_with_scores = list(zip(tst_ids[i], scores[i]))\n","            ids_with_scores = sorted(ids_with_scores, key=lambda x: x[1], reverse=True)\n","            if seq_len[i] < 5:\n","                group_num[0] += 1\n","            elif seq_len[i] >= 5 and seq_len[i] < 10:\n","                group_num[1] += 1\n","            elif seq_len[i] >= 10 and seq_len[i] < 20:\n","                group_num[2] += 1\n","            else:\n","                group_num[3] += 1\n","            shoot = list(map(lambda x: x[0], ids_with_scores[:5]))\n","            if pos_ids[i] in shoot:\n","                h5 += 1\n","                n5 += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","            shoot = list(map(lambda x: x[0], ids_with_scores[:10]))\n","            if pos_ids[i] in shoot:\n","                h10 += 1\n","                n10 += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","            shoot = list(map(lambda x: x[0], ids_with_scores[:20]))\n","            if pos_ids[i] in shoot:\n","                if seq_len[i] < 5:\n","                    group_h20[0] += 1\n","                    group_n20[0] += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","                elif seq_len[i] >= 5 and seq_len[i] < 10:\n","                    group_h20[1] += 1\n","                    group_n20[1] += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","                elif seq_len[i] >= 10 and seq_len[i] < 20:\n","                    group_h20[2] += 1\n","                    group_n20[2] += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","                else:\n","                    group_h20[3] += 1\n","                    group_n20[3] += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","                h20 += 1\n","                n20 += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","            shoot = list(map(lambda x: x[0], ids_with_scores[:50]))\n","            if pos_ids[i] in shoot:\n","                h50 += 1\n","                n50 += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","        return h5, n5, h10, n10, h20, n20, h50, n50, group_h20, group_n20, group_num\n","\n","    def save_history(self):\n","        if args.epoch == 0:\n","            return\n","\n","        if not os.path.exists('./Models/'):\n","                os.makedirs('./Models/')\n","\n","        if not os.path.exists('./History/'):\n","                os.makedirs('./History/')\n","\n","        with open('./History/' + args.save_path + '.his', 'wb') as fs:\n","            pickle.dump(self.metrics, fs)\n","\n","        content = {\n","            'encoder': self.encoder,\n","            'decoder': self.decoder,\n","            'recommender': self.recommender,\n","        }\n","        t.save(content, './Models/' + args.save_path + '.mod')\n","\n","        log('Model Saved: %s' % args.save_path)\n","\n","    def load_model(self):\n","        ckp = t.load('./Models/' + args.load_model + '.mod')\n","        self.encoder = ckp['encoder']\n","        self.decoder= ckp['decoder']\n","        self.recommender = ckp['recommender']\n","        self.opt = t.optim.Adam(\n","            [{\"params\": self.encoder.parameters()},\n","             {\"params\": self.decoder.parameters()},\n","             {\"params\": self.recommender.parameters()}],\n","            lr=args.lr, weight_decay=0\n","        )\n","\n","        with open('./History/' + args.load_model + '.his', 'rb') as fs:\n","            self.metrics = pickle.load(fs)\n","\n","        log('Model Loaded from ' + args.load_model)\n","\n","    def generate_recommendations(self, sequence):\n","      self.encoder.eval()\n","      self.recommender.eval()\n","\n","      with t.no_grad():\n","          sequence = sequence.cuda()\n","          item_emb, _ = self.encoder(self.handler.ii_adj)\n","          seq_emb = self.recommender(sequence, item_emb)\n","          seq_emb = seq_emb[:, -1, :]  # Get the last hidden state\n","          all_emb = item_emb.unsqueeze(0).expand(sequence.size(0), -1, -1)  # Expand item embeddings to match sequence size\n","          scores = t.sum(t.unsqueeze(seq_emb, 1) * all_emb, -1)  # Calculate scores\n","          recommendations = scores.argsort(descending=True)[:, :5]  # Get top 5 recommendations\n","\n","      return recommendations.cpu().numpy().tolist()\n","\n","if __name__ == '__main__':\n","    logger.saveDefault = True\n","    \n","    print(args)\n","\n","    log('Start')\n","    handler = DataHandler()\n","    handler.load_data()\n","    log('Load Data')\n","\n","    coach = Coach(handler)\n","    coach.run()\n","\n","    st.title(\"Sequence Recommendation System\")\n","\n","    sequence_input_str = st.text_input(\"Enter a sequence of item IDs separated by spaces:\")\n","    a=sequence_input_str.replace(\" \",\"\")\n","    if sequence_input_str:\n","        sequence_input = t.tensor([int(x) for x in sequence_input_str.split()]).unsqueeze(0)  # Convert input string to tensor\n","        recommendations = coach.generate_recommendations(sequence_input)\n","        df = pd.read_csv('/content/amazon.csv')\n","        st.write(\"inputed items:\")\n","        for i in a:\n","          st.write(df.iloc[int(i),5])\n","        st.write(\"Recommendations:\")\n","        for i in recommendations[0]:\n","            st.write(df.iloc[int(i),5])\n","\n","\n","```\n","\n"],"metadata":{"id":"i7S8aXb6Y3e9"}},{"cell_type":"markdown","source":["# creating app.py"],"metadata":{"id":"l-TmCRqEZmUl"}},{"cell_type":"code","source":["%%writefile /content/MAERec/app.py\n","import streamlit as st\n","import logger as logger\n","from params import args\n","from logger import log\n","from handler import *\n","from model import *\n","from utils import *\n","import numpy as np\n","import torch as t\n","import pandas as pd\n","import pickle\n","import sys\n","import os\n","\n","\n","gradient_bg = \"\"\"\n","    <style>\n","       [data-testid=\"stApp\"]{background: linear-gradient(135deg, rgba(114, 248, 255, 1), rgba(248, 248, 248, 1), rgba(255, 202, 250, 1), rgba(140, 248, 252, 1))}\n","\n","    </style>\n","\"\"\"\n","\n","st.markdown(gradient_bg, unsafe_allow_html=True)\n","\n","t.manual_seed(args.seed)\n","np.random.seed(args.seed)\n","\n","class Coach:\n","    def __init__(self, handler):\n","        self.handler = handler\n","\n","        log(f\"Users: {args.user}, Items(+1): {args.item}\")\n","        self.metrics = dict()\n","        mets = ['loss', 'loss_main', 'hr@10', 'ndcg@10']\n","        for met in mets:\n","            self.metrics['Train' + met] = list()\n","            self.metrics['Test' + met] = list()\n","\n","    def make_print(self, name, ep, reses, save):\n","        ret = 'Epoch %d/%d, %s: ' % (ep, args.epoch, name)\n","        for metric in reses:\n","            val = reses[metric]\n","            ret += '%s = %.4f, ' % (metric, val)\n","            tem = name + metric\n","            if save and tem in self.metrics:\n","                self.metrics[tem].append(val)\n","        ret = ret[:-2] + '                   '\n","        return ret\n","\n","    def run(self):\n","      self.prepare_model()\n","      log('Model Prepared')\n","      if args.load_model != None:\n","          self.load_model()\n","          stloc = len(self.metrics['Trainloss']) * args.test_frequency - (args.test_frequency - 1)\n","      else:\n","          stloc = 0\n","          log('Model Initialized')\n","\n","      bestRes = {'hr@10': 0, 'ndcg@10': 0}  # Initialize with initial values\n","\n","      reses = self.test_epoch()\n","      for ep in range(stloc, args.epoch):\n","          tst_flag = (ep % args.test_frequency == 0)\n","          reses = self.train_epoch()\n","          log(self.make_print('Train', ep, reses, tst_flag))\n","          sys.stdout.flush()\n","          if tst_flag:\n","              reses = self.test_epoch()\n","              log(self.make_print('Test', ep, reses, tst_flag))\n","              sys.stdout.flush()\n","              if bestRes is None or reses['hr@10'] > bestRes['hr@10']:\n","                  bestRes = reses\n","                  log(self.make_print('Best Result', args.epoch, bestRes, True), bold=True)\n","                  self.save_history()\n","          print()\n","\n","      reses = self.test_epoch()\n","      log(self.make_print('Test', args.epoch, reses, True))\n","      log(self.make_print('Best Result', args.epoch, bestRes, True), bold=True)\n","\n","    def prepare_model(self):\n","        self.encoder = Encoder().cuda()\n","        self.decoder = Decoder().cuda()\n","        self.recommender = SASRec().cuda()\n","        self.masker = RandomMaskSubgraphs()\n","        self.sampler = LocalGraph()\n","        self.opt = t.optim.Adam(\n","            [{\"params\": self.encoder.parameters()},\n","             {\"params\": self.decoder.parameters()},\n","             {\"params\": self.recommender.parameters()}],\n","            lr=args.lr, weight_decay=0\n","        )\n","\n","    def sample_pos_edges(self, masked_edges):\n","        return masked_edges[t.randperm(masked_edges.shape[0])[:args.con_batch]]\n","\n","    def sample_neg_edges(self, pos, dok):\n","        neg = []\n","        for u, v in pos:\n","            cu_neg = []\n","            num_samp = args.num_reco_neg // 2\n","            for i in range(num_samp):\n","                while True:\n","                    v_neg = np.random.randint(1, args.item)\n","                    if (u, v_neg) not in dok:\n","                        break\n","                cu_neg.append([u, v_neg])\n","            for i in range(num_samp):\n","                while True:\n","                    u_neg = np.random.randint(1, args.item)\n","                    if (u_neg, v) not in dok:\n","                        break\n","                cu_neg.append([u_neg, v])\n","            neg.append(cu_neg)\n","        return t.Tensor(neg).long()\n","\n","    def train_epoch(self):\n","        self.encoder.train()\n","        self.decoder.train()\n","        self.recommender.train()\n","        self.masker.train()\n","        self.sampler.train()\n","\n","        loss_his = []\n","        ep_loss, ep_loss_main, ep_loss_reco, ep_loss_mask = 0, 0, 0, 0\n","        trn_loader = self.handler.trn_loader\n","        steps = trn_loader.dataset.__len__() // args.trn_batch\n","\n","        for i, batch_data in enumerate(trn_loader):\n","\n","            if i % args.mask_steps == 0:\n","                sample_scr, candidates = self.sampler(self.handler.ii_adj_all_one, self.encoder.get_ego_embeds())\n","                masked_adj, masked_edg = self.masker(self.handler.ii_adj, candidates)\n","\n","            batch_data = [i.cuda() for i in batch_data]\n","            seq, pos, neg = batch_data\n","\n","            item_emb, item_emb_his = self.encoder(masked_adj)\n","            seq_emb = self.recommender(seq, item_emb)\n","            tar_msk = pos > 0\n","            loss_main = cross_entropy(seq_emb, item_emb[pos], item_emb[neg], tar_msk)\n","\n","            pos = self.sample_pos_edges(masked_edg)\n","            neg = self.sample_neg_edges(pos, self.handler.ii_dok)\n","            loss_reco = self.decoder(item_emb_his, pos, neg)\n","\n","            loss_regu = (calc_reg_loss(self.encoder) + calc_reg_loss(self.decoder) + calc_reg_loss(self.recommender)) * args.reg\n","\n","            loss = loss_main + loss_reco + loss_regu\n","            loss_his.append(loss_main)\n","\n","            if i % args.mask_steps == 0:\n","                reward = calc_reward(loss_his, args.eps)\n","                loss_mask = -sample_scr.mean() * reward\n","                ep_loss_mask += loss_mask\n","                loss_his = loss_his[-1:]\n","                loss += loss_mask\n","\n","            ep_loss += loss.item()\n","            ep_loss_main += loss_main.item()\n","            ep_loss_reco += loss_reco.item()\n","            log('Step %d/%d: loss = %.3f, loss_main = %.3f loss_regu = %.3f, loss_reco = %.3f        ' % (i, steps, loss, loss_main, loss_regu, loss_reco), save=False, oneline=True)\n","            sys.stdout.flush()\n","\n","            self.opt.zero_grad()\n","            loss.backward()\n","            self.opt.step()\n","\n","        ret = dict()\n","        ret['loss'] = ep_loss / steps\n","        ret['loss_main'] = ep_loss_main / steps\n","        ret['loss_reco'] = ep_loss_reco / steps\n","        ret['loss_mask'] = ep_loss_mask / (steps // args.mask_steps)\n","\n","        return ret\n","\n","    def test_epoch(self):\n","        self.encoder.eval()\n","        self.decoder.eval()\n","        self.recommender.eval()\n","        self.masker.eval()\n","        self.sampler.eval()\n","\n","        tst_loader = self.handler.tst_loader\n","        ep_h5, ep_n5, ep_h10, ep_n10, ep_h20, ep_n20, ep_h50, ep_n50  = [0] * 8\n","        group_h20 = [0] * 4\n","        group_n20 = [0] * 4\n","        group_num = [0] * 4\n","        num = tst_loader.dataset.__len__()\n","        steps = num // args.tst_batch\n","\n","        with t.no_grad():\n","            for i, batch_data in enumerate(tst_loader):\n","                batch_data = [i.cuda() for i in batch_data]\n","                seq, pos, neg = batch_data\n","                item_emb, item_emb_his = self.encoder(self.handler.ii_adj)\n","                seq_emb = self.recommender(seq, item_emb)\n","                seq_emb = seq_emb[:,-1,:] # (batch, 1, latdim)\n","                all_ids = t.cat([pos, neg], -1) # (batch, 100)\n","                all_emb = item_emb[all_ids] # (batch, 100, latdim)\n","                all_scr = t.sum(t.unsqueeze(seq_emb, 1) * all_emb, -1) # (batch, 100)\n","                seq_len = (seq > 0).cpu().numpy().sum(-1)\n","                h5, n5, h10, n10, h20, n20, h50, n50, gp_h20, gp_n20, gp_num= \\\n","                    self.calc_res(all_scr.cpu().numpy(), all_ids.cpu().numpy(), pos.cpu().numpy(), seq_len)\n","                ep_h5 += h5\n","                ep_n5 += n5\n","                ep_h10 += h10\n","                ep_n10 += n10\n","                ep_h20 += h20\n","                ep_n20 += n20\n","                ep_h50 += h50\n","                ep_n50 += n50\n","                for j in range(4):\n","                    group_h20[j] += gp_h20[j]\n","                    group_n20[j] += gp_n20[j]\n","                    group_num[j] += gp_num[j]\n","                log('Steps %d/%d: hr@10 = %.2f, ndcg@10 = %.2f          ' % (i, steps, h10, n10), save=False, oneline=True)\n","                sys.stdout.flush()\n","\n","        ep_h5 /= num\n","        ep_n5 /= num\n","        ep_h10 /= num\n","        ep_n10 /= num\n","        ep_h20 /= num\n","        ep_n20 /= num\n","        ep_h50 /= num\n","        ep_n50 /= num\n","\n","        for i in range(4):\n","            group_h20[i] /= group_num[i]\n","            group_n20[i] /= group_num[i]\n","\n","        ret = dict()\n","        ret['hr@10'] = ep_h10\n","        ret['ndcg@10'] = ep_n10\n","\n","        print(f'Test result: h5={ep_h5:.4f} n5={ep_n5:.4f} h10={ep_h10:.4f} n10={ep_n10:.4f} h20={ep_h20:.4f} n20={ep_n20:.4f} h50={ep_h50:.4f} n50={ep_n50:.4f}')\n","\n","        return ret\n","\n","    def calc_res(self, scores, tst_ids, pos_ids, seq_len):\n","        group_h20 = [0] * 4\n","        group_n20 = [0] * 4\n","        group_num = [0] * 4\n","        h5, n5, h10, n10, h20, n20, h50, n50 = [0] * 8\n","        for i in range(len(pos_ids)):\n","            ids_with_scores = list(zip(tst_ids[i], scores[i]))\n","            ids_with_scores = sorted(ids_with_scores, key=lambda x: x[1], reverse=True)\n","            if seq_len[i] < 5:\n","                group_num[0] += 1\n","            elif seq_len[i] >= 5 and seq_len[i] < 10:\n","                group_num[1] += 1\n","            elif seq_len[i] >= 10 and seq_len[i] < 20:\n","                group_num[2] += 1\n","            else:\n","                group_num[3] += 1\n","            shoot = list(map(lambda x: x[0], ids_with_scores[:5]))\n","            if pos_ids[i] in shoot:\n","                h5 += 1\n","                n5 += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","            shoot = list(map(lambda x: x[0], ids_with_scores[:10]))\n","            if pos_ids[i] in shoot:\n","                h10 += 1\n","                n10 += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","            shoot = list(map(lambda x: x[0], ids_with_scores[:20]))\n","            if pos_ids[i] in shoot:\n","                if seq_len[i] < 5:\n","                    group_h20[0] += 1\n","                    group_n20[0] += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","                elif seq_len[i] >= 5 and seq_len[i] < 10:\n","                    group_h20[1] += 1\n","                    group_n20[1] += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","                elif seq_len[i] >= 10 and seq_len[i] < 20:\n","                    group_h20[2] += 1\n","                    group_n20[2] += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","                else:\n","                    group_h20[3] += 1\n","                    group_n20[3] += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","                h20 += 1\n","                n20 += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","            shoot = list(map(lambda x: x[0], ids_with_scores[:50]))\n","            if pos_ids[i] in shoot:\n","                h50 += 1\n","                n50 += np.reciprocal(np.log2(shoot.index(pos_ids[i]) + 2))\n","        return h5, n5, h10, n10, h20, n20, h50, n50, group_h20, group_n20, group_num\n","\n","    def save_history(self):\n","        if args.epoch == 0:\n","            return\n","\n","        if not os.path.exists('./Models/'):\n","                os.makedirs('./Models/')\n","\n","        if not os.path.exists('./History/'):\n","                os.makedirs('./History/')\n","\n","        with open('./History/' + args.save_path + '.his', 'wb') as fs:\n","            pickle.dump(self.metrics, fs)\n","\n","        content = {\n","            'encoder': self.encoder,\n","            'decoder': self.decoder,\n","            'recommender': self.recommender,\n","        }\n","        t.save(content, './Models/' + args.save_path + '.mod')\n","\n","        log('Model Saved: %s' % args.save_path)\n","\n","    def load_model(self):\n","        ckp = t.load('./Models/' + args.load_model + '.mod')\n","        self.encoder = ckp['encoder']\n","        self.decoder= ckp['decoder']\n","        self.recommender = ckp['recommender']\n","        self.opt = t.optim.Adam(\n","            [{\"params\": self.encoder.parameters()},\n","             {\"params\": self.decoder.parameters()},\n","             {\"params\": self.recommender.parameters()}],\n","            lr=args.lr, weight_decay=0\n","        )\n","\n","        with open('./History/' + args.load_model + '.his', 'rb') as fs:\n","            self.metrics = pickle.load(fs)\n","\n","        log('Model Loaded from ' + args.load_model)\n","\n","    def generate_recommendations(self, sequence):\n","      self.encoder.eval()\n","      self.recommender.eval()\n","\n","      with t.no_grad():\n","          sequence = sequence.cuda()\n","          item_emb, _ = self.encoder(self.handler.ii_adj)\n","          seq_emb = self.recommender(sequence, item_emb)\n","          seq_emb = seq_emb[:, -1, :]  # Get the last hidden state\n","          all_emb = item_emb.unsqueeze(0).expand(sequence.size(0), -1, -1)  # Expand item embeddings to match sequence size\n","          scores = t.sum(t.unsqueeze(seq_emb, 1) * all_emb, -1)  # Calculate scores\n","          recommendations = scores.argsort(descending=True)[:, :5]  # Get top 5 recommendations\n","\n","      return recommendations.cpu().numpy().tolist()\n","\n","\n","def load_data():\n","    return pd.read_csv('/content/amazon.csv')\n","\n","def display_books(df, indices, title):\n","    st.title(title)\n","    for i in indices:\n","        book_link = df.iloc[i, 5]\n","        st.markdown(f'<div style=\"border: 1px solid grey; border-radius: 10px; padding: 10px;\"><a href=\"{book_link}\" style=\"font-weight: bold; color: black; text-decoration: none;\">{book_link}</a></div>', unsafe_allow_html=True)\n","\n","def display_images(df, indices, title):\n","    st.title(title)\n","    for i in indices:\n","        image_url = df.iloc[i, 4]\n","        st.image(image_url, caption=df.iloc[i, 1], width=200)\n","\n","if __name__ == '__main__':\n","    logger.saveDefault = True\n","\n","    print(args)\n","\n","    log('Start')\n","    handler = DataHandler()\n","    handler.load_data()\n","    log('Load Data')\n","\n","    coach = Coach(handler)\n","    coach.run()\n","\n","    st.title(\"Sequential Recommendation System\")\n","\n","    # Load the CSV file once\n","    df = load_data()\n","\n","    # Create a multiselect widget to display checkboxes with book names as options\n","    selected_books = st.multiselect(\"Select book(s):\", df.iloc[:, 1])\n","\n","    # Convert the selected books to their corresponding indices\n","    selected_books_indices = [df.index[df.iloc[:, 1] == book][0] for book in selected_books]\n","\n","    if selected_books_indices:\n","        # Convert the selected books' indices to a tensor\n","        sequence_input = t.tensor(selected_books_indices).unsqueeze(0)  # Convert input string to tensor\n","        recommendations = coach.generate_recommendations(sequence_input)\n","\n","        # Display selected books\n","        st.write(\"Input items:\")\n","        display_books(df, selected_books_indices, \"Selected Books\")\n","\n","        # Display recommendations\n","        st.write(\"Recommendations:\")\n","        display_books(df, recommendations[0], \"Recommended Books\")\n","\n","        # Display input images\n","        display_images(df, selected_books_indices, \"Images of Input Books\")\n","\n","        # Display output images\n","        display_images(df, recommendations[0], \"Images of Recommended Books\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKwme_FdW2Ug","outputId":"83bafd14-3cac-4160-983c-9c57c83a3f4e","executionInfo":{"status":"ok","timestamp":1724997443916,"user_tz":-330,"elapsed":608,"user":{"displayName":"Ruchita Bhogi","userId":"02603965711634267794"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/MAERec/app.py\n"]}]},{"cell_type":"markdown","source":["# **tunnel password**"],"metadata":{"id":"6NjXTkiK-zVc"}},{"cell_type":"code","source":["!wget -q -O - https://loca.lt/mytunnelpassword"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aZCAN_Gl9wE8","outputId":"33c7cc60-cf83-4b2d-b1ab-0f7e9262dc80","executionInfo":{"status":"ok","timestamp":1724997448474,"user_tz":-330,"elapsed":1342,"user":{"displayName":"Ruchita Bhogi","userId":"02603965711634267794"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["34.147.99.78"]}]},{"cell_type":"markdown","source":["# **Running app.py**"],"metadata":{"id":"jjR4sqRsbLVF"}},{"cell_type":"markdown","source":["**only displays output when this is running otherwise it shows 404,click your url and enter input seq**"],"metadata":{"id":"s0KX12ER_E__"}},{"cell_type":"code","source":["!streamlit run /content/MAERec/app.py & npx localtunnel --port 8501"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RFF7tNAFXKoL","outputId":"25270697-95b5-453a-96c3-baa1407232c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n","\u001b[0m\n","\u001b[0m\n","\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n","\u001b[0m\n","\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n","\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n","\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.147.99.78:8501\u001b[0m\n","\u001b[0m\n","\u001b[1G\u001b[0JNeed to install the following packages:\n","  localtunnel@2.0.2\n","Ok to proceed? (y) \u001b[20Gy\n","\u001b[K\u001b[?25hyour url is: https://quick-papayas-listen.loca.lt\n","Namespace(lr=0.001, seed=19260817, data='books', epoch=20, trn_batch=256, tst_batch=256, con_batch=2048, test_frequency=1, max_seq_len=50, num_reco_neg=40, reg=1e-06, ssl_reg=0.01, latdim=32, mask_depth=3, path_prob=0.5, num_attention_heads=4, num_gcn_layers=2, num_trm_layers=2, load_model=None, num_mask_cand=50, mask_steps=10, eps=0.2, attention_probs_dropout_prob=0.3, hidden_dropout_prob=0.3, save_path='tem')\n","2024-08-30 05:59:23.647953: Start\n","/content/MAERec/handler.py:52: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:641.)\n","  return t.sparse.FloatTensor(idxs, vals, shape).cuda()\n","2024-08-30 05:59:27.644201: Load Data\n","2024-08-30 05:59:27.644255: Users: 93043, Items(+1): 54756\n","2024-08-30 05:59:28.904244: Model Prepared\n","2024-08-30 05:59:28.904290: Model Initialized\n","Test result: h5=0.0896 n5=0.0542 h10=0.1461 n10=0.0725 h20=0.1634 n20=0.0771 h50=0.1670 n50=0.0778\n"]}]}]}